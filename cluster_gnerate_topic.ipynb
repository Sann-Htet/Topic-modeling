{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from attr import dataclass\n",
    "\n",
    "from haystack import Document\n",
    "import numpy as np\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from pydantic import BaseModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sympy import content\n",
    "from transformers import pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.retrievers.in_memory.embedding_retriever import InMemoryEmbeddingRetriever\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "\n",
    "EFS_PATH=\".\"\n",
    "doc_embedder = SentenceTransformersDocumentEmbedder()\n",
    "doc_embedder.warm_up()\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "@dataclass\n",
    "class RequestData:\n",
    "    id: str\n",
    "    description: str \n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=\"deep-learning-analytics/automatic-title-generation\", device=\"cuda\")\n",
    "def _generate_topic(document: str) -> list[dict[str, str]]:\n",
    "  return pipe(document)\n",
    "\n",
    "def _cluster_docs(request_data: list[dict], document_embeddings: dict[str, list[Document]], retriever: InMemoryEmbeddingRetriever) -> list:\n",
    "    clusters = []\n",
    "    visited_documents = set()\n",
    "\n",
    "    for doc in document_embeddings[\"documents\"]:\n",
    "        if doc.meta[\"id\"] in visited_documents:\n",
    "            continue\n",
    "\n",
    "        cluster = [doc.meta[\"id\"]]\n",
    "        visited_documents.add(doc.meta[\"id\"])\n",
    "\n",
    "        similar_documents = retriever.run(doc.embedding, top_k=50)[\"documents\"]\n",
    "        for similar_doc in similar_documents:\n",
    "            if similar_doc.id not in visited_documents and similar_doc.score >= 0.7:\n",
    "                cluster.append(similar_doc.id)\n",
    "                visited_documents.add(similar_doc.id)\n",
    "\n",
    "        if cluster:\n",
    "            clusters.append(cluster)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def _generate_embeddings(request_data: list[dict], document_store:InMemoryDocumentStore) ->  tuple[dict[str, list[Document]], InMemoryEmbeddingRetriever]:\n",
    "    documents = [Document(content=d[\"description\"],meta={\"id\":d[\"id\"]}) for d in request_data]\n",
    "    document_store.write_documents(documents=documents)\n",
    "\n",
    "    document_embeddings = doc_embedder.run(documents)\n",
    "    retriever = InMemoryEmbeddingRetriever(document_store)\n",
    "    return document_embeddings, retriever\n",
    "\n",
    "def cluster_and_generate_topics(request_data: list[dict], document_embeddings: dict[str, list[Document]], retriever: InMemoryEmbeddingRetriever) -> list:\n",
    "\n",
    "    clusters = _cluster_docs(request_data, document_embeddings, retriever)\n",
    "\n",
    "    id_to_description = {request[\"id\"]: request[\"description\"] for request in request_data}\n",
    "\n",
    "    clustered_topics = []\n",
    "    for cluster_ids in clusters:\n",
    "        cluster_documents = [id_to_description[request_id] for request_id in cluster_ids]\n",
    "        joined_text = \" \".join(cluster_documents)\n",
    "        generated_topic = _generate_topic(joined_text)[0][\"generated_text\"]\n",
    "        topic_data = {\n",
    "            \"results\": [],\n",
    "            \"topic\": generated_topic,\n",
    "        }\n",
    "\n",
    "        for request_id in cluster_ids:\n",
    "          topic_data[\"results\"].append({\n",
    "              \"request_id\": request_id,\n",
    "            #   \"request_sequence\": 0,\n",
    "              \"description\": id_to_description[request_id],\n",
    "          })\n",
    "        clustered_topics.append(topic_data)\n",
    "\n",
    "    filepath = Path(f\"{EFS_PATH}/clustered_topics.json\")\n",
    "    with filepath.open(mode=\"w\") as jsonfile:\n",
    "        json.dump(clustered_topics, jsonfile)\n",
    "\n",
    "    return clustered_topics\n",
    "\n",
    "\n",
    "def find_similar_topic(\n",
    "    document_store: InMemoryDocumentStore, new_request: dict[str, str]\n",
    ") -> dict[str, str]:\n",
    "    description = new_request[\"description\"]\n",
    "    request_id = new_request[\"id\"]\n",
    "\n",
    "    documents = [Document(content=description, meta={\"id\": request_id})]\n",
    "    document_store.write_documents(documents=documents, policy=DuplicatePolicy.SKIP)\n",
    "\n",
    "    document_embeddings = doc_embedder.run(documents)\n",
    "\n",
    "    request_embed = [document_embeddings[\"documents\"][0].embedding]\n",
    "\n",
    "    file_path = Path(f\"{EFS_PATH}/clustered_topics.json\")\n",
    "    with file_path.open(mode=\"r\") as jsondata:\n",
    "        topic_data = json.loads(jsondata.read())\n",
    "\n",
    "    max_similarity = -1\n",
    "    max_topic = None\n",
    "    topic_item = None\n",
    "\n",
    "    for item in topic_data:\n",
    "        request_doc = document_store.filter_documents(\n",
    "            filters={\n",
    "                \"field\": \"meta.id\",\n",
    "                \"operator\": \"==\",\n",
    "                \"value\": item[\"results\"][0][\"request_id\"],\n",
    "            }\n",
    "        )\n",
    "        embedding_vector = [request_doc[0].embedding]\n",
    "\n",
    "        similarity = cosine_similarity(embedding_vector, request_embed)\n",
    "\n",
    "        if similarity > 0.7 and similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            max_topic = item[\"topic\"]\n",
    "            topic_item = item\n",
    "\n",
    "    if max_topic is None:\n",
    "        max_topic = _generate_topic(description)[0][\"generated_text\"]\n",
    "        topic_data.append(\n",
    "            {\n",
    "                \"results\": [\n",
    "                    {\n",
    "                        \"request_id\": request_id,\n",
    "                        # \"request_sequence\": 0,\n",
    "                        \"description\": description,\n",
    "                    }\n",
    "                ],\n",
    "                \"topic\": max_topic,\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        for item in topic_data:\n",
    "            if (\n",
    "                item[\"results\"][0][\"request_id\"]\n",
    "                == topic_item[\"results\"][0][\"request_id\"]\n",
    "            ):\n",
    "                item[\"results\"].append(\n",
    "                    {\n",
    "                        \"request_id\": request_id,\n",
    "                        # \"request_sequence\": 0,\n",
    "                        \"description\": description,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    filepath = Path(f\"{EFS_PATH}/clustered_topics.json\")\n",
    "    with filepath.open(mode=\"w\") as jsonfile:\n",
    "        json.dump(topic_data, jsonfile)\n",
    "\n",
    "    return {\n",
    "        \"topic\": max_topic,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"FOIALog_FY15.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"Request ID\":\"id\",\"Request Description \": \"description\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_data_frame = df[[\"id\",\"description\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "request_data = request_data_frame.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc65b40b5304d47a53b3cbdb4f36fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedings, retriever = _generate_embeddings(request_data=request_data,document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_and_generate_topics(request_data=request_data,document_embeddings=embedings, retriever)\n",
    "clustered = _cluster_docs(request_data, embedings, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/media/nozander/c8a3e945-09e9-4905-9f64-2b593e6a1297/FOIA-Kit/foiakit-datascience/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1726 > 512). Running this sequence through the model will result in indexing errors\n",
      "/run/media/nozander/c8a3e945-09e9-4905-9f64-2b593e6a1297/FOIA-Kit/foiakit-datascience/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "c=  cluster_and_generate_topics(request_data, embedings, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {\"id\": \"0211\",\n",
    "            \"description\": \"Requesting records relating to Department of Defense Instruction (DODI) 5240.02, Department of Defense Counterintelligence Program.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar_topic(document_store, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_json = Path(\"clustered_topics.json\")\n",
    "with topic_json.open(mode=\"r\") as jsondata:\n",
    "    topic_data = json.loads(jsondata.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "def extract_pii_entities(data: str) -> dict[str, list[dict[str, str]]]:\n",
    "\n",
    "    model = GLiNER.from_pretrained(\"urchade/gliner_multi_pii-v1\").to(\"cpu\")\n",
    "\n",
    "    labels = [\n",
    "        \"person\",\n",
    "        \"age\",\n",
    "        \"organization\",\n",
    "        \"phone number\",\n",
    "        \"address\",\n",
    "        \"passport number\",\n",
    "        \"email\",\n",
    "        \"credit card number\",\n",
    "        \"social security number\",\n",
    "        \"health insurance id number\",\n",
    "        \"date of birth\",\n",
    "        \"mobile phone number\",\n",
    "        \"bank account number\",\n",
    "        \"social_security_number\",\n",
    "        \"medication\",\n",
    "        \"cpf\",\n",
    "        \"tax identification number\",\n",
    "        \"driver's license number\",\n",
    "        \"medical condition\",\n",
    "        \"identity card number\",\n",
    "        \"national id number\",\n",
    "        \"ip address\",\n",
    "        \"email address\",\n",
    "        \"credit card expiration date\",\n",
    "        \"username\",\n",
    "        \"health insurance number\",\n",
    "        \"registration number\",\n",
    "        \"student id number\",\n",
    "        \"insurance number\",\n",
    "        \"flight number\",\n",
    "        \"landline phone number\",\n",
    "        \"blood type\",\n",
    "        \"cvv\",\n",
    "        \"reservation number\",\n",
    "        \"digital signature\",\n",
    "        \"social media handle\",\n",
    "        \"license plate number\",\n",
    "        \"cnpj\",\n",
    "        \"postal code\",\n",
    "        \"passport_number\",\n",
    "        \"serial number\",\n",
    "        \"vehicle registration number\",\n",
    "        \"credit card brand\",\n",
    "        \"fax number\",\n",
    "        \"visa number\",\n",
    "        \"insurance company\",\n",
    "        \"identity document number\",\n",
    "        \"transaction number\",\n",
    "        \"national health insurance number\",\n",
    "        \"cvc\",\n",
    "        \"birth certificate number\",\n",
    "        \"train ticket number\",\n",
    "        \"passport expiration date\",\n",
    "        \"biometric data\",\n",
    "    ]\n",
    "\n",
    "    entities = model.predict_entities(data, labels)\n",
    "\n",
    "    return {\n",
    "        \"entities\": [\n",
    "            {\"entity_group\": entity[\"label\"], \"start\": entity[\"start\"], \"end\": entity[\"end\"], \"text\": entity[\"text\"]}\n",
    "            for entity in entities\n",
    "        ],\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
